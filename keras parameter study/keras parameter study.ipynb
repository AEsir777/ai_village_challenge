{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7626df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.7.0-py3-none-any.whl (4.7 MB)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.1.0-py3-none-any.whl (5.8 kB)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (3.19.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.27.1)\n",
      "Collecting etils[epath]\n",
      "  Downloading etils-0.9.0-py3-none-any.whl (140 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (4.64.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: toml in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.21.5)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.11.0-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: six in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n",
      "Collecting importlib_resources\n",
      "  Downloading importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: zipp in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from etils[epath]->tensorflow_datasets) (3.7.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from etils[epath]->tensorflow_datasets) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tqdm->tensorflow_datasets) (0.4.4)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=ca2a4be7b2301d5f9bf341853ee8506b419d7c627f5791cbdf90de35a5b77b6e\n",
      "  Stored in directory: c:\\users\\aesir\\appdata\\local\\pip\\cache\\wheels\\e1\\e8\\83\\ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "Successfully built promise\n",
      "Installing collected packages: etils, importlib-resources, absl-py, termcolor, tensorflow-metadata, promise, dill, tensorflow-datasets\n",
      "Successfully installed absl-py-1.3.0 dill-0.3.6 etils-0.9.0 importlib-resources-5.10.0 promise-2.3 tensorflow-datasets-4.7.0 tensorflow-metadata-1.11.0 termcolor-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bad259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.42.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (61.2.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.10.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.33.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2021.10.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aesir\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.4)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow-intel, tensorflow\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-22.10.26 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 keras-2.11.0 libclang-14.0.6 oauthlib-3.2.2 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.27.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e8a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e61df04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\AEsir\\tensorflow_datasets\\fashion_mnist\\3.0.1...\n",
      "Dataset fashion_mnist downloaded and prepared to C:\\Users\\AEsir\\tensorflow_datasets\\fashion_mnist\\3.0.1. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\n",
    "train_ds, test_ds = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61304253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='fashion_mnist',\n",
       "    full_name='fashion_mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
       "    \"\"\",\n",
       "    homepage='https://github.com/zalandoresearch/fashion-mnist',\n",
       "    data_path='C:\\\\Users\\\\AEsir\\\\tensorflow_datasets\\\\fashion_mnist\\\\3.0.1',\n",
       "    file_format=tfrecord,\n",
       "    download_size=29.45 MiB,\n",
       "    dataset_size=36.42 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{DBLP:journals/corr/abs-1708-07747,\n",
       "      author    = {Han Xiao and\n",
       "                   Kashif Rasul and\n",
       "                   Roland Vollgraf},\n",
       "      title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n",
       "                   Algorithms},\n",
       "      journal   = {CoRR},\n",
       "      volume    = {abs/1708.07747},\n",
       "      year      = {2017},\n",
       "      url       = {http://arxiv.org/abs/1708.07747},\n",
       "      archivePrefix = {arXiv},\n",
       "      eprint    = {1708.07747},\n",
       "      timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},\n",
       "      biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},\n",
       "      bibsource = {dblp computer science bibliography, https://dblp.org}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85c20f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = metadata.features['label'].names\n",
    "num_test_examples = metadata.splits['test'].num_examples\n",
    "num_train_examples = metadata.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c91a1001",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def normalize(images, labels):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images /= 255\n",
    "    return images, labels\n",
    "\n",
    "train_ds = train_ds.map(normalize)\n",
    "test_ds = test_ds.map(normalize)\n",
    "\n",
    "train_ds = train_ds.cache()\n",
    "test_ds = test_ds.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73eb01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a557dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c3ddc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_batch = train_ds.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
    "test_batch = test_ds.cache().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a1c13be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 6s 2ms/step - loss: 0.4941 - accuracy: 0.8270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27b12c8ae20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set training epochs set to 1\n",
    "basic_model.fit(train_batch, epochs=1, steps_per_epoch=math.ceil(num_train_examples / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "527460e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4352 - accuracy: 0.8428\n",
      "0.8428000211715698\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = basic_model.evaluate(test_batch, steps = math.ceil(num_test_examples / BATCH_SIZE))\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5661c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy will be lower than 5 epochs which is about 87% in test accuracy \n",
    "#  and 89% in training accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7fef315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_neurons(neuron_num):\n",
    "    print('number of neurons in the dense layer: {}'.format(neuron_num))\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Dense(neuron_num, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "    model.fit(train_batch, epochs=5, steps_per_epoch=math.ceil(num_train_examples / BATCH_SIZE))\n",
    "    test_losses, test_accuracy = model.evaluate(test_batch, steps = math.ceil(num_test_examples / BATCH_SIZE))\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5750face",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of neurons in the dense layer: 10\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.6986 - accuracy: 0.7664\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4715 - accuracy: 0.8376\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4424 - accuracy: 0.8456\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4336 - accuracy: 0.8487\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4223 - accuracy: 0.8535\n",
      "313/313 [==============================] - 0s 920us/step - loss: 0.4726 - accuracy: 0.8358\n",
      "number of neurons in the dense layer: 100\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4987 - accuracy: 0.8245\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3703 - accuracy: 0.8669\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3408 - accuracy: 0.8753\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3157 - accuracy: 0.8847\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2973 - accuracy: 0.8906\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3682 - accuracy: 0.8685\n",
      "number of neurons in the dense layer: 256\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4788 - accuracy: 0.8307\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3576 - accuracy: 0.8704\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3201 - accuracy: 0.8825\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3023 - accuracy: 0.8890\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2808 - accuracy: 0.8945\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3551 - accuracy: 0.8748\n",
      "number of neurons in the dense layer: 512\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4719 - accuracy: 0.8322\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3578 - accuracy: 0.8683\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3251 - accuracy: 0.8804\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2972 - accuracy: 0.8900\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2788 - accuracy: 0.8966\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3534 - accuracy: 0.8741\n"
     ]
    }
   ],
   "source": [
    "neuron_array = [10, 100, 256, 512]\n",
    "accuracy = []\n",
    "for num in neuron_array:\n",
    "    accuracy.append(diff_neurons(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "63d31bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num of neuron</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.8358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.8685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256</td>\n",
       "      <td>0.8748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>512</td>\n",
       "      <td>0.8741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num of neuron  accuracy\n",
       "0             10    0.8358\n",
       "1            100    0.8685\n",
       "2            256    0.8748\n",
       "3            512    0.8741"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'num of neuron': neuron_array, 'accuracy': accuracy}\n",
    "pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca3c97c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different neurons in additional layer\n",
    "def diff_neurons_add_layer(neuron_num):\n",
    "    print('number of neurons in the dense layer: {}'.format(neuron_num))\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(neuron_num, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "    model.fit(train_batch, epochs=5, steps_per_epoch=math.ceil(num_train_examples / BATCH_SIZE))\n",
    "    test_losses, test_accuracy = model.evaluate(test_batch, steps = math.ceil(num_test_examples / BATCH_SIZE))\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ec7d81e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of neurons in the dense layer: 10\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5550 - accuracy: 0.8041\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3847 - accuracy: 0.8607\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3437 - accuracy: 0.8748\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3164 - accuracy: 0.8841\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3029 - accuracy: 0.8912\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3578 - accuracy: 0.8697\n",
      "number of neurons in the dense layer: 100\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4864 - accuracy: 0.8267\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3616 - accuracy: 0.8663\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3264 - accuracy: 0.8807\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3008 - accuracy: 0.8874\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2866 - accuracy: 0.8939\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3384 - accuracy: 0.8773\n",
      "number of neurons in the dense layer: 256\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4789 - accuracy: 0.8259\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3594 - accuracy: 0.8682\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3228 - accuracy: 0.8803\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3020 - accuracy: 0.8880\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2815 - accuracy: 0.8943\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3497 - accuracy: 0.8727\n",
      "number of neurons in the dense layer: 512\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4725 - accuracy: 0.8292\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3612 - accuracy: 0.8674\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3219 - accuracy: 0.8812\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2999 - accuracy: 0.8883\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2798 - accuracy: 0.8947\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3510 - accuracy: 0.8788\n"
     ]
    }
   ],
   "source": [
    "neuron_array = [10, 100, 256, 512]\n",
    "accuracy = []\n",
    "for num in neuron_array:\n",
    "    accuracy.append(diff_neurons_add_layer(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65d2d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86b516ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num of neuron</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.8697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.8773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256</td>\n",
       "      <td>0.8727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>512</td>\n",
       "      <td>0.8788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num of neuron  accuracy\n",
       "0             10    0.8697\n",
       "1            100    0.8773\n",
       "2            256    0.8727\n",
       "3            512    0.8788"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'num of neuron': neuron_array, 'accuracy': accuracy}\n",
    "pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "023c3731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3.2903 - accuracy: 0.7208\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.6254 - accuracy: 0.7808\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5618 - accuracy: 0.7999\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5230 - accuracy: 0.8151\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5028 - accuracy: 0.8238\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.5248 - accuracy: 0.8184\n",
      "The test accuracy is: 0.8184000253677368\n"
     ]
    }
   ],
   "source": [
    "# don't normalize the pixel values\n",
    "train_no_norm, test_no_norm = dataset['train'], dataset['test']\n",
    "train_no_norm = train_no_norm.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
    "test_no_norm = test_no_norm.cache().batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "no_norm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "])\n",
    "no_norm_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "         metrics=['accuracy'])\n",
    "no_norm_model.fit(train_no_norm, epochs=5, steps_per_epoch=math.ceil(num_train_examples / BATCH_SIZE))\n",
    "test_losses, test_accuracy = no_norm_model.evaluate(test_no_norm, steps = math.ceil(num_test_examples / BATCH_SIZE))\n",
    "print('The test accuracy is: {}'. format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37089e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>basic model</td>\n",
       "      <td>0.8722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>epochs=1</td>\n",
       "      <td>0.8270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>don't normalize</td>\n",
       "      <td>0.8184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          scenario accuracy\n",
       "0      basic model   0.8722\n",
       "1         epochs=1   0.8270\n",
       "2  don't normalize   0.8184"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'scenario': ['basic model', 'epochs=1', \"don't normalize\"],\n",
    "       'accuracy': ['0.8722', '0.8270', '0.8184']}\n",
    "pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52959743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf0be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296cecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296676d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e81ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf81e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947bb2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e36dad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddff24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6de66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904fa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
